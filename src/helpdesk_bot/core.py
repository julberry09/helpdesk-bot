# core.py
import os
import json
import logging
import time as _time
from typing import TypedDict, List, Dict, Any, Optional
from pathlib import Path

from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader, CSVLoader, TextLoader, Docx2txtLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, END
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings

# =============================================================
# 1. ê³µí†µ ì„¤ì • / í™˜ê²½ ë³€ìˆ˜
# =============================================================
load_dotenv()

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("helpdesk-bot")
if not logger.handlers:
    LOG_DIR = Path("./logs"); LOG_DIR.mkdir(parents=True, exist_ok=True)
    class _ConsoleFormatter(logging.Formatter):
        def format(self, record):
            base = {"level": record.levelname, "name": record.name, "msg": record.getMessage()}
            if hasattr(record, "extra_data"):
                base.update(record.extra_data)
            return json.dumps(base, ensure_ascii=False)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(_ConsoleFormatter())
    file_handler = logging.FileHandler(LOG_DIR / "app.log", encoding="utf-8")
    file_handler.setFormatter(_ConsoleFormatter())
    logger.setLevel(logging.INFO)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# Azure OpenAI í™˜ê²½ë³€ìˆ˜
AOAI_ENDPOINT    = os.getenv("AOAI_ENDPOINT", "")
AOAI_API_KEY     = os.getenv("AOAI_API_KEY", "")
AOAI_API_VERSION = os.getenv("AOAI_API_VERSION", "2024-10-21")
AOAI_DEPLOY_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI", "gpt-4o-mini")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O", "gpt-4o")
AOAI_DEPLOY_EMBED_3_SMALL = os.getenv("AOAI_DEPLOY_EMBED_3_SMALL", "text-embedding-3-small")

# ê²½ë¡œ
KB_DIR = Path("./kb")
INDEX_DIR = Path("./index")
INDEX_NAME = "faiss_index"

# ìƒ˜í”Œ ë°ì´í„°
OWNER_FALLBACK = {
    "ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìžê´€ë¦¬": {"owner": "í™ê¸¸ë™", "email": "owner.hr@example.com", "phone": "010-1234-5678"},
    "ìž¬ë¬´ì‹œìŠ¤í…œ-ì •ì‚°í™”ë©´": {"owner": "ê¹€ìž¬ë¬´", "email": "owner.fa@example.com", "phone": "010-2222-3333"},
    "í¬í„¸-ê³µì§€ìž‘ì„±": {"owner": "ë°•ìš´ì˜", "email": "owner.ops@example.com", "phone": "010-9999-0000"},
}
EMPLOYEE_DIR = {
    "kim.s": {"name": "ê¹€ì„ ë‹ˆ", "dept": "ITìš´ì˜", "phone": "010-1111-2222", "status": "active"},
    "lee.a": {"name": "ì´ì•ŒíŒŒ", "dept": "ë³´ì•ˆ", "phone": "010-3333-4444", "status": "active"},
}

# =============================================================
# 2. RAG ìœ í‹¸ë¦¬í‹°
# =============================================================
def _make_embedder() -> AzureOpenAIEmbeddings:
    return AzureOpenAIEmbeddings(
        azure_deployment=AOAI_DEPLOY_EMBED_3_SMALL,
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
        api_version=AOAI_API_VERSION,
    )

def _load_docs_from_kb() -> List[Document]:
    docs: List[Document] = []
    if not KB_DIR.exists():
        KB_DIR.mkdir(parents=True, exist_ok=True)
    for p in KB_DIR.rglob("*"):
        if p.is_file():
            try:
                suf = p.suffix.lower()
                if suf == ".pdf": docs.extend(PyPDFLoader(str(p)).load())
                elif suf == ".csv": docs.extend(CSVLoader(file_path=str(p), encoding="utf-8").load())
                elif suf in [".txt", ".md"]: docs.extend(TextLoader(str(p), encoding="utf-8").load())
                elif suf == ".docx": docs.extend(Docx2txtLoader(str(p)).load())
            except Exception as e:
                logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {p} - {e}")
    return docs

def build_or_load_vectorstore() -> FAISS:
    embed = _make_embedder()
    if (INDEX_DIR / f"{INDEX_NAME}.faiss").exists():
        return FAISS.load_local(str(INDEX_DIR / INDEX_NAME), embeddings=embed, allow_dangerous_deserialization=True)

    raw_docs = _load_docs_from_kb()
    if not raw_docs:
        seed_text = """ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ì•ˆë‚´
- ID ë°œê¸‰: ì‹ ê·œ ìž…ì‚¬ìžëŠ” HR í¬í„¸ì—ì„œ 'ê³„ì • ì‹ ì²­' ì–‘ì‹ì„ ì œì¶œ. ìŠ¹ì¸ í›„ ITê°€ ê³„ì • ìƒì„±.
- ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”: SSO í¬í„¸ì˜ 'ë¹„ë°€ë²ˆí˜¸ ìž¬ì„¤ì •' ê¸°ëŠ¥ ì‚¬ìš©. ë³¸ì¸ì¸ì¦ í•„ìš”.
- ë‹´ë‹¹ìž ì¡°íšŒ: í¬í„¸ ìƒë‹¨ ê²€ìƒ‰ì°½ì— í™”ë©´/ë©”ë‰´ëª…ì„ ìž…ë ¥í•˜ë©´ ë‹´ë‹¹ìž ì¹´ë“œê°€ í‘œì‹œë¨."""
        raw_docs = [Document(page_content=seed_text, metadata={"source": "seed-faq.txt"})]

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    chunks = splitter.split_documents(raw_docs)
    INDEX_DIR.mkdir(parents=True, exist_ok=True)
    vs = FAISS.from_documents(chunks, embed)
    vs.save_local(str(INDEX_DIR / INDEX_NAME))
    return vs

_vectorstore: Optional[FAISS] = None
def retriever(k: int = 4):
    global _vectorstore
    if _vectorstore is None:
        _vectorstore = build_or_load_vectorstore()
    return _vectorstore.as_retriever(search_kwargs={"k": k})

def make_llm(model: str = AOAI_DEPLOY_GPT4O_MINI, temperature: float = 0.2) -> AzureChatOpenAI:
    if not (AOAI_ENDPOINT and AOAI_API_KEY):
        raise RuntimeError("AOAI_ENDPOINT/AOAI_API_KEY ë¯¸ì„¤ì •. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”.")
    return AzureChatOpenAI(
        azure_deployment=model,
        api_version=AOAI_API_VERSION,
        azure_endpoint=AOAI_ENDPOINT,
        api_key=AOAI_API_KEY,
        temperature=temperature,
    )

# =============================================================
# 3. LangGraph (ë„êµ¬ + ë…¸ë“œ)
# =============================================================
class BotState(TypedDict):
    question: str; intent: str; result: str
    sources: List[Dict[str, Any]]; tool_output: Dict[str, Any]

def tool_reset_password(payload: Dict[str, Any]) -> Dict[str, Any]:
    user = payload.get("user") or ""
    found = EMPLOYEE_DIR.get(user)
    if not found:
        return {"ok": False, "message": "ì‚¬ë²ˆ/ê³„ì •ì´ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
    return {"ok": True, "message": f"{found['name']}ë‹˜ì˜ ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨ ì•ˆë‚´", "steps": ["SSO í¬í„¸ ì ‘ì† > ë¹„ë°€ë²ˆí˜¸ ìž¬ì„¤ì •", "ë³¸ì¸ì¸ì¦", "ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •"]}

def tool_request_id(payload: Dict[str, Any]) -> Dict[str, Any]:
    return {"ok": True, "message": "ID ë°œê¸‰ ì‹ ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.", "ticket": f"REQ-{int(_time.time())}"}

def tool_owner_lookup(payload: Dict[str, Any]) -> Dict[str, Any]:
    screen = payload.get("screen") or ""
    info = OWNER_FALLBACK.get(screen)
    if not info:
        return {"ok": False, "message": f"'{screen}' ë‹´ë‹¹ìž ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
    return {"ok": True, "screen": screen, "owner": info}

def node_classify(state: BotState) -> BotState:
    llm = make_llm()
    sys_prompt = ("ë‹¹ì‹ ì€ ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ë¼ìš°í„°ìž…ë‹ˆë‹¤. ì‚¬ìš©ìž ìž…ë ¥ì„ reset_password, request_id, owner_lookup, rag_qa ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”. JSON(intent, arguments)ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”.")
    msg = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": state["question"]}]
    out = llm.invoke(msg).content
    intent, args = "rag_qa", {}
    try:
        data = json.loads(out)
        intent = data.get("intent", "rag_qa")
        args = data.get("arguments", {}) or {}
    except Exception: pass
    return {**state, "intent": intent, "tool_output": args}

def node_reset_pw(state: BotState) -> BotState: return {**state, "tool_output": tool_reset_password(state.get("tool_output", {}))}
def node_request_id(state: BotState) -> BotState: return {**state, "tool_output": tool_request_id(state.get("tool_output", {}))}
def node_owner_lookup(state: BotState) -> BotState: return {**state, "tool_output": tool_owner_lookup(state.get("tool_output", {}))}

def node_rag(state: BotState) -> BotState:
    docs = retriever(k=4).get_relevant_documents(state["question"])
    context = "\n\n".join([f"[{i+1}] {d.page_content[:1200]}" for i, d in enumerate(docs)])
    sources = [{"index": i+1, "source": d.metadata.get("source","unknown"), "page": d.metadata.get("page")} for i,d in enumerate(docs)]
    llm = make_llm(model=AOAI_DEPLOY_GPT4O)
    sys_prompt = "ë„ˆëŠ” ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ìƒë‹´ì›ì´ë‹¤. ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ìž‘ì„±í•´ë¼."
    user_prompt = f"ì§ˆë¬¸:\n{state['question']}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}"
    out = llm.invoke([{"role":"system","content":sys_prompt},{"role":"user","content":user_prompt}]).content
    return {**state, "result": out, "sources": sources}

def node_finalize(state: BotState) -> BotState:
    if state["intent"] in ["reset_password", "request_id", "owner_lookup"]:
        res = state.get("tool_output", {})
        if state["intent"] == "reset_password":
            text = f"âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i,s in enumerate(res.get("steps", []))) if res.get("ok") else f"â—{res.get('message','ì‹¤íŒ¨')}"
        elif state["intent"] == "request_id":
            text = f"ðŸ†” ID ë°œê¸‰ ì‹ ì²­\nìƒíƒœ: {'ì ‘ìˆ˜ë¨' if res.get('ok') else 'ì‹¤íŒ¨'}\ní‹°ì¼“: {res.get('ticket','-')}"
        else:
            text = f"ðŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ìž\n- ì´ë¦„: {res.get('owner', {}).get('owner')}\n- ì´ë©”ì¼: {res.get('owner', {}).get('email')}\n- ì—°ë½ì²˜: {res.get('owner', {}).get('phone')}" if res.get("ok") else f"â—{res.get('message','ì¡°íšŒ ì‹¤íŒ¨')}"
        return {**state, "result": text}
    return state

def build_graph():
    g = StateGraph(BotState)
    g.add_node("classify", node_classify); g.add_node("reset_password", node_reset_pw); g.add_node("request_id", node_request_id)
    g.add_node("owner_lookup", node_owner_lookup); g.add_node("rag", node_rag); g.add_node("finalize", node_finalize)
    g.set_entry_point("classify")
    g.add_conditional_edges("classify", lambda s: s["intent"], {"reset_password":"finalize", "request_id":"finalize", "owner_lookup":"finalize", "rag":"rag"})
    g.add_edge("finalize", END); g.add_edge("rag", END)
    return g.compile()

_graph = None
def pipeline(question: str) -> Dict[str, Any]:
    global _graph
    logger.info("pipeline_in", extra={"extra_data": {"q": question}})
    if _graph is None: _graph = build_graph()
    state: BotState = {"question": question, "intent":"", "result":"", "sources":[], "tool_output":{}}
    out = _graph.invoke(state)
    logger.info("pipeline_out", extra={"extra_data": {"intent": out.get("intent","")}})
    return out