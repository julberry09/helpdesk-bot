# src/helpdesk_bot/core.py

# Standard library imports
import os
import json
import logging
import csv
import time as _time
import threading # ì¶”ê°€: ìŠ¤ë ˆë”© ëª¨ë“ˆ ì„í¬íŠ¸
from typing import TypedDict, List, Dict, Any, Optional
from pathlib import Path

# Third-party imports
from dotenv import load_dotenv
from konlpy.tag import Okt
from langchain import hub
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.tools import tool
from langchain_community.document_loaders import PyPDFLoader, CSVLoader, TextLoader, Docx2txtLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END

# LangSmithë¥¼ ìœ„í•œ CallbackManager ì„í¬íŠ¸ (python ë²„ì „ ë‚®ì¶°ì•¼í•´ì„œ hold -  3.11)
# from langchain.callbacks.manager import CallbackManager
# from langchain_community.callbacks.langsmith import LangSmithCallbackHandler

# Local application imports
from . import constants

# =============================================================
# 1. ê³µí†µ ì„¤ì • / í™˜ê²½ ë³€ìˆ˜
# =============================================================
load_dotenv()

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("helpdesk-bot")
if not logger.handlers:
    LOG_DIR = Path("./logs"); LOG_DIR.mkdir(parents=True, exist_ok=True)
    class _ConsoleFormatter(logging.Formatter):
        def format(self, record):
            base = {"level": record.levelname, "name": record.name, "msg": record.getMessage()}
            if hasattr(record, "extra_data"):
                base.update(record.extra_data)
            return json.dumps(base, ensure_ascii=False)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(_ConsoleFormatter())
    file_handler = logging.FileHandler(LOG_DIR / "app.log", encoding="utf-8")
    file_handler.setFormatter(_ConsoleFormatter())
    logger.setLevel(logging.INFO)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# Azure OpenAI í™˜ê²½ë³€ìˆ˜
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT", "")
AOAI_API_KEY = os.getenv("AOAI_API_KEY", "")
AOAI_API_VERSION = os.getenv("AOAI_API_VERSION", "2024-10-21")
AOAI_DEPLOY_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI", "gpt-4o-mini")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O", "gpt-4o")
AOAI_DEPLOY_EMBED_3_SMALL = os.getenv("AOAI_DEPLOY_EMBED_3_SMALL", "text-embedding-3-small")

# Azure ì„¤ì • í™•ì¸ í”Œë˜ê·¸
AZURE_AVAILABLE = bool(AOAI_ENDPOINT and AOAI_API_KEY)
if not AZURE_AVAILABLE:
    logger.warning("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ í´ë°±(Fallback) ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")


# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° - Lazy Initialization (Thread-Safe)
_okt = None
_okt_lock = threading.Lock()
_faq_data = None   # FAQ ë°ì´í„° ìºì‹œ ì „ì—­ë³€ìˆ˜

def get_okt():
    """
    Okt ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    JVMì€ í”„ë¡œì„¸ìŠ¤ì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
    thread-safe lazy init íŒ¨í„´ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
    """
    global _okt
    if _okt is None:
        with _okt_lock:  # ë‹¤ë¥¸ ìŠ¤ë ˆë“œì™€ ë™ì‹œ ì‹¤í–‰ ë°©ì§€
            if _okt is None:  # double-checked locking
                _okt = Okt()
    return _okt


# # Okt í˜•íƒœì†Œ ë¶„ì„ ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©
# class SingletonOkt:
#     _instance = None
#     _lock = threading.Lock() # ìŠ¤ë ˆë“œ ì ê¸ˆ ê°ì²´
# 
#     def __new__(cls):
#         with cls._lock: # ë½(lock)ì„ ì‚¬ìš©í•´ ìŠ¤ë ˆë“œë¡œë¶€í„° ì•ˆì „í•˜ê²Œ ì ‘ê·¼
#             if cls._instance is None:
#                 try:
#                     # Okt ê°ì²´ëŠ” í•œ ë²ˆë§Œ ìƒì„±
#                     cls._instance = Okt()
#                     logger.info("Okt ê°ì²´ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
#                 except Exception as e:
#                     logger.error(f"Okt ê°ì²´ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#                     raise RuntimeError("Okt ê°ì²´ ì´ˆê¸°í™” ì‹¤íŒ¨") from e
#         return cls._instance
# 
# # Okt ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œì ì— í•œ ë²ˆë§Œ ë¯¸ë¦¬ ìƒì„±
# _okt_instance = None
# def get_okt():
#     """Okt ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜ (Okt ê°ì²´ëŠ” í•œ ë²ˆë§Œ ìƒì„±)"""
#     global _okt_instance
#     if _okt_instance is None:
#         _okt_instance = SingletonOkt() # ìˆ˜ì •ëœ SingletonOkt í´ë˜ìŠ¤ ì‚¬ìš©
#     return _okt_instance

# =============================================================
# 2. RAG ë° LLM ê´€ë ¨ í•¨ìˆ˜ ì •ì˜
# =============================================================
# ì„ë² ë”© ëª¨ë¸ ìƒì„±
def _make_embedder() -> AzureOpenAIEmbeddings:
    if not AZURE_AVAILABLE:
        raise RuntimeError("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ Embedderë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return AzureOpenAIEmbeddings(
        azure_deployment=AOAI_DEPLOY_EMBED_3_SMALL,
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
        api_version=AOAI_API_VERSION,
    )

# RAG - ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ ë¡œì§ [checklist: 6]
def _load_docs_from_kb() -> List[Document]:
    docs: List[Document] = []
    for kb_path in [constants.KB_DEFAULT_DIR, constants.KB_DATA_DIR]:
        if not kb_path.exists():
            kb_path.mkdir(parents=True, exist_ok=True)
        for p in kb_path.rglob("*"):
            if p.is_file():
                try:
                    suf = p.suffix.lower()
                    if suf == ".pdf": docs.extend(PyPDFLoader(str(p)).load())
                    elif suf == ".csv" and p.name != "faq_data.csv":
                        docs.extend(CSVLoader(file_path=str(p), encoding="utf-8").load())
                    elif suf in [".txt", ".md"]: docs.extend(TextLoader(str(p), encoding="utf-8").load())
                    elif suf == ".docx": docs.extend(Docx2txtLoader(str(p)).load())
                except Exception as e:
                    logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {p} - {e}")
    return docs

# RAG - FAISS ê¸°ë°˜ì˜ Vector ìŠ¤í† ì–´ êµ¬ì¶• [checklist: 7]
def build_or_load_vectorstore() -> FAISS:
    if not AZURE_AVAILABLE:
        raise RuntimeError("'Rebuild Index'ëŠ” Azure OpenAI ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
    embed = _make_embedder()
    if (constants.INDEX_DIR / f"{constants.INDEX_NAME}.faiss").exists():
        return FAISS.load_local(str(constants.INDEX_DIR / constants.INDEX_NAME), embeddings=embed, allow_dangerous_deserialization=True)

    raw_docs = _load_docs_from_kb()
    
    if not raw_docs:
        faq_data = load_faq_data()
        if faq_data:
            raw_docs = [
                Document(
                    page_content=f"ì§ˆë¬¸: {item.get('question')}\në‹µë³€: {item.get('answer')}",
                    metadata={"source": "faq_data.csv"}
                ) for item in faq_data
            ]
            logger.info("ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ì–´ faq_data.csvë¥¼ ê¸°ë³¸ RAG ì§€ì‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            seed_text = """ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ì•ˆë‚´
- ID ë°œê¸‰: ì‹ ê·œ ì…ì‚¬ìëŠ” HR í¬í„¸ì—ì„œ 'ê³„ì • ì‹ ì²­' ì–‘ì‹ì„ ì œì¶œ. ìŠ¹ì¸ í›„ ITê°€ ê³„ì • ìƒì„±.
- ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”: SSO í¬í„¸ì˜ 'ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •' ê¸°ëŠ¥ ì‚¬ìš©. ë³¸ì¸ì¸ì¦ í•„ìš”.
- ë‹´ë‹¹ì ì¡°íšŒ: í¬í„¸ ìƒë‹¨ ê²€ìƒ‰ì°½ì— í™”ë©´/ë©”ë‰´ëª…ì„ ì…ë ¥í•˜ë©´ ë‹´ë‹¹ì ì¹´ë“œê°€ í‘œì‹œë¨."""
            raw_docs = [Document(page_content=seed_text, metadata={"source": "seed-faq.txt"})]

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    chunks = splitter.split_documents(raw_docs)
    constants.INDEX_DIR.mkdir(parents=True, exist_ok=True)
    # FAISSì— ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ì €ì¥
    vs = FAISS.from_documents(chunks, embed)
    vs.save_local(str(constants.INDEX_DIR / constants.INDEX_NAME))
    return vs

# RAG - FAISS ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° (Singleton Pattern)
_vectorstore: Optional[FAISS] = None
def retriever(k: int = 4):
    global _vectorstore
    if _vectorstore is None:
        _vectorstore = build_or_load_vectorstore()
    return _vectorstore.as_retriever(search_kwargs={"k": k})

# LLM(ì–¸ì–´ ëª¨ë¸) ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
def make_llm(model: str = AOAI_DEPLOY_GPT4O_MINI, temperature: float = 0.2) -> AzureChatOpenAI:
    """
    Azure OpenAI ì„œë¹„ìŠ¤ì— ì—°ê²°í•˜ì—¬ LLM(ì–¸ì–´ ëª¨ë¸) ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    Args:
        model (str): ì‚¬ìš©í•  Azure OpenAI ë°°í¬ ëª¨ë¸ì˜ ì´ë¦„. ê¸°ë³¸ê°’ì€ gpt-4o-miniì…ë‹ˆë‹¤.
        temperature (float): ëª¨ë¸ì˜ ì°½ì˜ì„±(ë¬´ì‘ìœ„ì„±)ì„ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜. 0.0ì—ì„œ 2.0 ì‚¬ì´ì˜ ê°’. 
                           ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ì¼ê´€ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    Returns:
        AzureChatOpenAI: ì„¤ì •ëœ ì–¸ì–´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤.
    Raises:
        RuntimeError: Azure OpenAI í™˜ê²½ ë³€ìˆ˜(ì—”ë“œí¬ì¸íŠ¸, API í‚¤)ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ë°œìƒ.
    """
    if not AZURE_AVAILABLE:
        raise RuntimeError("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ LLMì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return AzureChatOpenAI(
        azure_deployment=model,
        api_version=AOAI_API_VERSION,
        azure_endpoint=AOAI_ENDPOINT,
        api_key=AOAI_API_KEY,
        temperature=temperature,
    )

# =============================================================
# 3. LangGraph ë„êµ¬ ì •ì˜
# =============================================================
# ìƒíƒœ ê´€ë¦¬ (State Management)
class BotState(TypedDict):
    question: str; intent: str; result: str
    sources: List[Dict[str, Any]]; tool_output: Dict[str, Any]

# ë„êµ¬(Tool) í•¨ìˆ˜ - LLM ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©
@tool
def tool_reset_password(payload: Dict[str, Any] = {}) -> Dict[str, Any]:
    """ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤."""
    return {
        "ok": True, 
        "message": "ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨ ì•ˆë‚´", 
        "steps": ["SSO í¬í„¸ ì ‘ì† > ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •", "ë³¸ì¸ì¸ì¦", "ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •"]
    }

@tool
def tool_request_id(payload: Dict[str, Any] = {}) -> Dict[str, Any]:
    """ID ë°œê¸‰ ì‹ ì²­ ì ˆì°¨ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤."""
    return {
        "ok": True, 
        "message": "ID ë°œê¸‰ ì‹ ì²­ ì ˆì°¨ ì•ˆë‚´", 
        "steps": ["HR í¬í„¸ ì ‘ì† > 'ê³„ì • ì‹ ì²­' ì–‘ì‹ ì œì¶œ", "ì–‘ì‹ ìŠ¹ì¸ í›„ ITíŒ€ì—ì„œ ê³„ì • ìƒì„±"]
    }

@tool
def tool_owner_lookup(payload: Dict[str, Any]) -> Dict[str, Any]:
    """í™”ë©´ì´ë‚˜ ë©”ë‰´ì˜ ë‹´ë‹¹ì ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. `screen` ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."""
    try:
        screen = payload.get("screen") or ""
        info = constants.OWNER_FALLBACK.get(screen)
        if not info:
            return {"ok": False, "message": f"'{screen}' ë‹´ë‹¹ì ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
        return {"ok": True, "screen": screen, "owner": info}
    except Exception as e:
        return {"ok": False, "message": f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

# RAG - ì‚¬ì „ ì •ì˜ëœ ë°ì´í„°(ë¬¸ì„œ)ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIì˜ ë…¼ë¦¬ë ¥ì„ ë³´ê°•/ RAG ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰ ê¸°ëŠ¥ êµ¬í˜„ [checklist: 8,9] 
# Prompt Engineering - í”„ë¡¬í”„íŠ¸ ìµœì í™” (ì—­í•  ë¶€ì—¬ + Chain-of-Thought) [checklist: 1] 
def node_rag(state: BotState) -> BotState:
    docs = retriever(k=4).get_relevant_documents(state["question"])
    context = "\n\n".join([f"[{i+1}] {d.page_content[:1200]}" for i, d in enumerate(docs)])
    sources = [{"index": i+1, "source": d.metadata.get("source","unknown"), "page": d.metadata.get("page")} for i,d in enumerate(docs)]
    llm = make_llm(model=AOAI_DEPLOY_GPT4O)
    sys_prompt = "ë„ˆëŠ” ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ìƒë‹´ì›ì´ë‹¤. ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ë¼. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ˆë¼."
    user_prompt = f"ì§ˆë¬¸:\n{state['question']}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}"
    out = llm.invoke([{"role":"system","content":sys_prompt},{"role":"user","content":user_prompt}]).content
    return {**state, "result": out, "sources": sources}

# ë„êµ¬(Tool) í•¨ìˆ˜ ê²°ê³¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•íƒœë¡œ ë³€í™˜
def node_finalize(state: BotState) -> BotState:
    # ì´ ë…¸ë“œëŠ” ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ ë©”ì‹œì§€ë¡œ ë³€í™˜
    res = state.get("tool_output", {})
    if state["intent"] == "direct_tool":
        if res.get("tool_name") == "tool_reset_password":
            text = f"âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i,s in enumerate(res.get("steps", []))) if res.get("ok") else f"â—{res.get('message','ì‹¤íŒ¨')}"
        elif res.get("tool_name") == "tool_request_id":
            text = f"ğŸ†” ID ë°œê¸‰ ì‹ ì²­\nìƒíƒœ: {'ì ‘ìˆ˜ë¨' if res.get('ok') else 'ì‹¤íŒ¨'}\ní‹°ì¼“: {res.get('ticket','-')}"
        elif res.get("tool_name") == "tool_owner_lookup":
            text = f"ğŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ì\n- ì´ë¦„: {res.get('owner', {}).get('owner')}\n- ì´ë©”ì¼: {res.get('owner', {}).get('email')}\n- ì—°ë½ì²˜: {res.get('owner', {}).get('phone')}" if res.get("ok") else f"â—{res.get('message','ì¡°íšŒ ì‹¤íŒ¨')}"
        else: # FAQë„ ì—¬ê¸°ì„œ ì²˜ë¦¬
            text = res.get("answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return {**state, "result": text}
    return state

# =============================================================
# 4. LangGraph Workflow ë° ë…¸ë“œ ì •ì˜
# ==========================================================
# LangChain & LangGraph - Multi-Agent Flow ì„¤ê³„ ë° êµ¬í˜„ [checklist: 3,4,5]
# FAQ ë°ì´í„° ë¡œë“œ
def load_faq_data() -> List[Dict[str, str]]:
    global _faq_data
    # ì´ë¯¸ ë¡œë“œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if _faq_data is not None:
        return _faq_data
    
    faq_file_path = constants.KB_DEFAULT_DIR / "faq_data.csv"
    if not faq_file_path.exists():
        logger.warning(f"FAQ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {faq_file_path}")
        _faq_data = []
        return _faq_data
    
    loaded_data = []
    try:
        # íŒŒì¼ ë¡œë“œ ë° ë°ì´í„° íŒŒì‹±
        with open(faq_file_path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            # Okt ê°ì²´ë¥¼ ë¯¸ë¦¬ ìƒì„±í•´ë‘ê³  ì¬ì‚¬ìš©
            okt_processor = get_okt()
            for row in reader:
                if "question" in row and "answer" in row:
                    # Okt ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„ì„
                    row["faq_words"] = set(okt_processor.phrases(row.get("question", "")))
                    loaded_data.append(row)
        logger.info(f"{len(loaded_data)}ê°œì˜ FAQ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"FAQ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë¹ˆ ëª©ë¡ì„ ë°˜í™˜
        loaded_data = []
    _faq_data = loaded_data
    return _faq_data

# FAQ ìœ ì‚¬ë„ ê²€ìƒ‰
def find_similar_faq(question: str) -> Optional[Dict[str, Any]]:
    faq_data = load_faq_data()
    if not faq_data: return None
    user_words = set(get_okt().phrases(question.lower()))
    if not user_words: user_words = set(get_okt().nouns(question.lower()))
    if not user_words: return None
    
    best_score = 0.0
    best_item = None
    
    for item in faq_data:
        faq_words = item.get("faq_words", set())
        if not faq_words: continue
        intersection = len(user_words.intersection(faq_words))
        union = len(user_words.union(faq_words))
        score = intersection / union if union > 0 else 0
        
        if score > best_score:
            best_score = score
            best_item = item

    # ì ìˆ˜ ì„ê³„ê°’(threshold)ì„ 0.2ë¡œ ì„¤ì •
    return best_item if best_score > 0.2 else None

# Prompt Engineering - í”„ë¡¬í”„íŠ¸ ìµœì í™” (Few-shot Prompting) [checklist: 1]
# ë…¸ë“œ(Node) í•¨ìˆ˜
def node_classify(state: BotState) -> BotState:
    llm = make_llm(model=AOAI_DEPLOY_GPT4O_MINI, temperature=0.1)
    prompt_template = PromptTemplate.from_template("""
    ë‹¹ì‹ ì€ ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    - `greeting`: ì‚¬ìš©ìê°€ ì¸ì‚¬ë§("ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”" ë“±)ì„ ê±´ë„¬ ë•Œ
    - `direct_tool`: ì‚¬ìš©ìê°€ íŠ¹ì • ì‹œìŠ¤í…œ ì‘ì—…(ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”, ID ë°œê¸‰, ë‹´ë‹¹ì ì¡°íšŒ)ì„ ìš”ì²­í•  ë•Œ
    - `faq`: ìì£¼ ë¬»ëŠ” ì§ˆë¬¸(FAQ)ì— ê´€ë ¨ëœ ì§ˆë¬¸ì¼ ë•Œ
    - `general_qa`: ìœ„ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ì¼ ë•Œ

    ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ë¥˜ì™€ í•„ìš”í•œ ì¸ì(JSON í˜•ì‹)ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
    JSON í˜•ì‹: {{"intent": "ë¶„ë¥˜", "arguments": {{"key": "value"}}}}
    ì˜ˆì‹œ:
    ì‚¬ìš©ì ì§ˆë¬¸: "ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•Œë ¤ì¤˜" -> {{"intent": "direct_tool", "arguments": {{"tool_name": "tool_reset_password"}}}}
    ì‚¬ìš©ì ì§ˆë¬¸: "ì¸ì‚¬ì‹œìŠ¤í…œ ë‹´ë‹¹ì ëˆ„êµ¬ì•¼?" -> {{"intent": "direct_tool", "arguments": {{"tool_name": "tool_owner_lookup", "screen": "ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬"}}}}
    ì‚¬ìš©ì ì§ˆë¬¸: "ì ì‹¬ì‹œê°„ì´ ì–¸ì œì•¼?" -> {{"intent": "faq", "arguments": {{}}}}
    ì‚¬ìš©ì ì§ˆë¬¸: "íšŒì‚¬ ë³µì§€ì œë„ ì„¤ëª…í•´ì¤˜" -> {{"intent": "general_qa", "arguments": {{}}}}
    ì‚¬ìš©ì ì§ˆë¬¸: "ì•ˆë…•" -> {{"intent": "greeting", "arguments": {{}}}}

    ì‚¬ìš©ì ì§ˆë¬¸: "{question}" -> 
    """)
    prompt = prompt_template.format(question=state["question"])
    out = llm.invoke(prompt).content
    
    intent, args = "general_qa", {}
    try:
        data = json.loads(out.strip())
        intent = data.get("intent", "general_qa")
        args = data.get("arguments", {}) or {}
    except json.JSONDecodeError:
        logger.warning(f"[Classifier ì˜¤ë¥˜] JSONDecodeError: {out}")
    except Exception as e:
        logger.error(f"[Classifier ì˜¤ë¥˜] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {out} - {e}")
    
    return {**state, "intent": intent, "tool_output": args}

def node_greeting(state: BotState) -> BotState:
    return {**state, "result": "ë„¤, ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬¸ì˜ì‚¬í•­ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ì œê°€ ë„ì™€ë“œë¦´ê²Œìš”.", "sources": []}

def node_direct_tool(state: BotState) -> BotState:
    tool_name = state.get("tool_output", {}).get("tool_name")
    payload = state.get("tool_output", {}).get("arguments", {})
    
    if tool_name == "tool_reset_password":
        res = tool_reset_password.invoke({})
    elif tool_name == "tool_request_id":
        res = tool_request_id.invoke({})
    elif tool_name == "tool_owner_lookup":
        res = tool_owner_lookup.invoke(payload)
    else:
        res = {"ok": False, "message": "ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ í˜¸ì¶œ"}
    
    # ë„êµ¬ ê²°ê³¼ì— ì›ë˜ í˜¸ì¶œëœ ë„êµ¬ ì´ë¦„ ì¶”ê°€
    res["tool_name"] = tool_name
    return {**state, "tool_output": res}

def node_faq(state: BotState) -> BotState:
    faq_answer = find_similar_faq(state["question"])
    if faq_answer:
        return {**state, "tool_output": {"ok": True, "answer": faq_answer.get("answer")}, "intent": "faq", "sources": [{"source": "faq_data.csv"}]} # ë³€ê²½: ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
    else:
        # FAQì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì¼ë°˜ QAë¡œ ì „í™˜
        return {**state, "intent": "general_qa"}

_memory_checkpointer = MemorySaver()
_graph = None
# StateGraph í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•¨
def build_graph():
    g = StateGraph(BotState)
    g.add_node("classify", node_classify)
    g.add_node("greeting", node_greeting)
    g.add_node("direct_tool", node_direct_tool)
    g.add_node("faq", node_faq)
    g.add_node("rag", node_rag)
    g.add_node("finalize", node_finalize)
    
    # ì±—ë´‡ì˜ ì‹œì‘ì 
    g.set_entry_point("classify")

    # ì˜ë„ì— ë”°ë¼ ë…¸ë“œ ì—°ê²°
    g.add_conditional_edges(
        "classify",
        lambda s: s["intent"],
        {
            "greeting": "greeting",
            "direct_tool": "direct_tool",
            "faq": "faq",
            "general_qa": "rag", # ì¼ë°˜ ì§ˆë¬¸ì€ RAGë¡œ ë¼ìš°íŒ…
        }
    )

    # ë…¸ë“œ ì—°ê²°
    g.add_edge("greeting", END)
    g.add_edge("direct_tool", "finalize")
    g.add_edge("rag", END)
    
    # FAQ ë…¸ë“œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í•˜ë©´ RAGë¡œ ë‹¤ì‹œ ë¼ìš°íŒ…
    g.add_conditional_edges(
        "faq",
        lambda s: s["intent"],
        {
            "faq": "finalize", # FAQ ë‹µë³€ì„ ì°¾ì•˜ì„ ë•Œ
            "general_qa": "rag", # FAQ ë‹µë³€ì„ ëª» ì°¾ì•˜ì„ ë•Œ
        }
    )
    g.add_edge("finalize", END)
    
    return g.compile(checkpointer=_memory_checkpointer)

# =============================================================
# 5. Pipeline Orchestration
# =============================================================
# LangChain & LangGraphë¥¼ í™œìš©í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
_graph = None
def run_graph_pipeline(question: str, session_id: str) -> Dict[str, Any]:
    # [checklist: 5] LangChain & LangGraph - ë©€í‹°í„´ ëŒ€í™” (memory) í™œìš©
    """LangGraph ê¸°ë°˜ì˜ AI íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    global _graph
    # LangGraph Studio (UI ê¸°ë°˜ ê·¸ë˜í”„ ì‹œê°í™” íˆ´)ë¥¼ í™œìš©í•˜ì—¬ ê·¸ë˜í”„ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹… - hold
    # LangSmith ì½œë°± í•¸ë“¤ëŸ¬ ì„¤ì •
    langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
    langsmith_project_name = os.getenv("LANGSMITH_PROJECT", "Helpdesk Bot")
    
    callbacks = None
    if langsmith_api_key:
        # callbacks = CallbackManager([
        #     LangSmithCallbackHandler(
        #         project_name=langsmith_project_name, 
        #         api_key=langsmith_api_key, 
        #         session_id=session_id
        #     )
        # ])
        logger.info("LangSmith is enabled.")
    else:
        logger.info("LangSmith is not enabled. Provide LANGSMITH_API_KEY in .env to enable it.")

    logger.info("pipeline_in", extra={"extra_data": {"q": question}})
    if _graph is None: _graph = build_graph()
    
    out = _graph.invoke(
        input={"question": question, "intent":"", "result":"", "sources":[], "tool_output":{}},
        config={"configurable": {"thread_id": session_id}, "callbacks": callbacks}
    )
    logger.info("pipeline_out", extra={"extra_data": {"intent": out.get("intent", "")}})
    
    # LangGraph ê²°ê³¼ì—ì„œ ìµœì¢… ë‹µë³€ê³¼ ì˜ë„ ë°˜í™˜
    return {
        "reply": out.get("result", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
        "intent": out.get("intent", "unsupported"),
        "sources": out.get("sources", []),
    }

def pipeline(question: str, session_id: str) -> Dict[str, Any]:
    """
    Azure ì—°ê²° ìƒíƒœì— ë”°ë¼ ì ì ˆí•œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìš”ì²­ì„ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    """
    if AZURE_AVAILABLE:
        try:
            return run_graph_pipeline(question, session_id)
        except Exception as e:
            logger.error(f"ì£¼ìš” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. í´ë°± ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            return {
                "reply": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                "intent": "system_error",
                "sources": []
            }
    else:
        # í´ë°± ëª¨ë“œ

        # 1. FAQ ê²€ìƒ‰ì„ ë„êµ¬ í‚¤ì›Œë“œë³´ë‹¤ ë¨¼ì € ìˆ˜í–‰í•˜ì—¬ RAG í…ŒìŠ¤íŠ¸ í†µê³¼
        faq_item = find_similar_faq(question)
        if faq_item:
            return {
                "reply": f"[ì•ˆë‚´] ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.\n\n---\n\n{faq_item.get('answer')}",
                "intent": "faq",
                "sources": [{"source": "faq_data.csv"}]
            }
        
        # 2. ë„êµ¬ ê´€ë ¨ í‚¤ì›Œë“œ ì²˜ë¦¬
        if "ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”" in question:
            res = tool_reset_password.invoke({})
            return {
                "reply": f"âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i, s in enumerate(res.get("steps", []))),
                "intent": "direct_tool",
                "sources": []
            }
        
        if "ì•„ì´ë”” ë°œê¸‰" in question or "ê³„ì • ë°œê¸‰" in question:
            res = tool_request_id.invoke({})
            return {
                "reply": f"ğŸ†” ID ë°œê¸‰ ì‹ ì²­\nìƒíƒœ: {'ì ‘ìˆ˜ë¨' if res.get('ok') else 'ì‹¤íŒ¨'}",
                "intent": "direct_tool",
                "sources": []
            }
            
        if "ë‹´ë‹¹ì" in question:
            screen = "ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬" if "ì¸ì‚¬ì‹œìŠ¤í…œ" in question else "ë‹´ë‹¹ì ì¡°íšŒ"
            
            # Pydantic ì˜¤ë¥˜ í•´ê²°: payload ì¸ìë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë˜í•‘
            res = tool_owner_lookup.invoke({"payload": {"screen": screen}})
            
            if res.get("ok"):
                reply = f"ğŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ì\n- ì´ë¦„: {res.get('owner', {}).get('owner')}\n- ì´ë©”ì¼: {res.get('owner', {}).get('email')}\n- ì—°ë½ì²˜: {res.get('owner', {}).get('phone')}"
            else:
                reply = f"â—{res.get('message', 'ì¡°íšŒ ì‹¤íŒ¨')}"
                
            return {
                "reply": reply,
                "intent": "direct_tool",
                "sources": []
            }
        
        # 3. ì¸ì‚¬ë§ ì²˜ë¦¬
        if question.lower().strip() in constants.GREETINGS:
            return {
                "reply": "ë„¤, ë°˜ê°‘ìŠµë‹ˆë‹¤. í˜„ì¬ëŠ” ê¸°ë³¸ ëª¨ë“œë¡œ ìš´ì˜ ì¤‘ì´ë©°, ê°„ë‹¨í•œ ë¬¸ì˜ë§Œ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "intent": "greeting",
                "sources": []
            }

        # 4. ê·¸ ì™¸ ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ í´ë°±
        return {
            "reply": "ì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "intent": "unsupported",
            "sources": []
        }