# src/helpdesk_bot/core.py

import os
import json
import logging
import csv
import time as _time
from typing import TypedDict, List, Dict, Any, Optional
from pathlib import Path

from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader, CSVLoader, TextLoader, Docx2txtLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, END
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langgraph.checkpoint.memory import MemorySaver
from konlpy.tag import Okt

from . import constants

# =============================================================
# 1. ê³µí†µ ì„¤ì • / í™˜ê²½ ë³€ìˆ˜
# =============================================================
load_dotenv()

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("helpdesk-bot")
if not logger.handlers:
    LOG_DIR = Path("./logs"); LOG_DIR.mkdir(parents=True, exist_ok=True)
    class _ConsoleFormatter(logging.Formatter):
        def format(self, record):
            base = {"level": record.levelname, "name": record.name, "msg": record.getMessage()}
            if hasattr(record, "extra_data"):
                base.update(record.extra_data)
            return json.dumps(base, ensure_ascii=False)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(_ConsoleFormatter())
    file_handler = logging.FileHandler(LOG_DIR / "app.log", encoding="utf-8")
    file_handler.setFormatter(_ConsoleFormatter())
    logger.setLevel(logging.INFO)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# Azure OpenAI í™˜ê²½ë³€ìˆ˜
AOAI_ENDPOINT    = os.getenv("AOAI_ENDPOINT", "")
AOAI_API_KEY     = os.getenv("AOAI_API_KEY", "")
AOAI_API_VERSION = os.getenv("AOAI_API_VERSION", "2024-10-21")
AOAI_DEPLOY_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI", "gpt-4o-mini")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O", "gpt-4o")
AOAI_DEPLOY_EMBED_3_SMALL = os.getenv("AOAI_DEPLOY_EMBED_3_SMALL", "text-embedding-3-small")

# Azure ì„¤ì • í™•ì¸ í”Œë˜ê·¸
AZURE_AVAILABLE = bool(AOAI_ENDPOINT and AOAI_API_KEY)
if not AZURE_AVAILABLE:
    logger.warning("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ í´ë°±(Fallback) ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt()

# =============================================================
# 4. Fallback & Main Pipelines 
# =============================================================
# ì„ë² ë”© ëª¨ë¸ ìƒì„±
def _make_embedder() -> AzureOpenAIEmbeddings:
    if not AZURE_AVAILABLE:
        raise RuntimeError("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ Embedderë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return AzureOpenAIEmbeddings(
        azure_deployment=AOAI_DEPLOY_EMBED_3_SMALL,
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
        api_version=AOAI_API_VERSION,
    
# RAG - ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ ë¡œì§ [checklist: 6] 
def _load_docs_from_kb() -> List[Document]:
    docs: List[Document] = []
    for kb_path in [KB_DEFAULT_DIR, KB_DATA_DIR]:
        if not kb_path.exists():
            kb_path.mkdir(parents=True, exist_ok=True)
        for p in kb_path.rglob("*"):
            if p.is_file():
                try:
                    suf = p.suffix.lower()
                    if suf == ".pdf": docs.extend(PyPDFLoader(str(p)).load())
                    elif suf == ".csv" and p.name != "faq_data.csv":
                        docs.extend(CSVLoader(file_path=str(p), encoding="utf-8").load())
                    elif suf in [".txt", ".md"]: docs.extend(TextLoader(str(p), encoding="utf-8").load())
                    elif suf == ".docx": docs.extend(Docx2txtLoader(str(p)).load())
                except Exception as e:
                    logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {p} - {e}")
    return docs

# RAG - FAISS ê¸°ë°˜ì˜ Vector ìŠ¤í† ì–´ êµ¬ì¶• [checklist: 7] 
def build_or_load_vectorstore() -> FAISS:
    if not AZURE_AVAILABLE:
        raise RuntimeError("'Rebuild Index'ëŠ” Azure OpenAI ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
    embed = _make_embedder()
    if (INDEX_DIR / f"{INDEX_NAME}.faiss").exists():
        return FAISS.load_local(str(INDEX_DIR / INDEX_NAME), embeddings=embed, allow_dangerous_deserialization=True)

    raw_docs = _load_docs_from_kb()
    
    if not raw_docs:
        faq_data = load_faq_data()
        if faq_data:
            raw_docs = [
                Document(
                    page_content=f"ì§ˆë¬¸: {item.get('question')}\në‹µë³€: {item.get('answer')}",
                    metadata={"source": "faq_data.csv"}
                ) for item in faq_data
            ]
            logger.info("ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ì–´ faq_data.csvë¥¼ ê¸°ë³¸ RAG ì§€ì‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            seed_text = """ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ì•ˆë‚´
- ID ë°œê¸‰: ì‹ ê·œ ì…ì‚¬ìëŠ” HR í¬í„¸ì—ì„œ 'ê³„ì • ì‹ ì²­' ì–‘ì‹ì„ ì œì¶œ. ìŠ¹ì¸ í›„ ITê°€ ê³„ì • ìƒì„±.
- ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”: SSO í¬í„¸ì˜ 'ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •' ê¸°ëŠ¥ ì‚¬ìš©. ë³¸ì¸ì¸ì¦ í•„ìš”.
- ë‹´ë‹¹ì ì¡°íšŒ: í¬í„¸ ìƒë‹¨ ê²€ìƒ‰ì°½ì— í™”ë©´/ë©”ë‰´ëª…ì„ ì…ë ¥í•˜ë©´ ë‹´ë‹¹ì ì¹´ë“œê°€ í‘œì‹œë¨."""
            raw_docs = [Document(page_content=seed_text, metadata={"source": "seed-faq.txt"})]

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    chunks = splitter.split_documents(raw_docs)
    INDEX_DIR.mkdir(parents=True, exist_ok=True)
    # FAISSì— ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ì €ì¥
    vs = FAISS.from_documents(chunks, embed)
    vs.save_local(str(INDEX_DIR / INDEX_NAME))
    return vs

# RAG - FAISS ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° (Singleton Pattern)
_vectorstore: Optional[FAISS] = None
def retriever(k: int = 4):
    global _vectorstore
    if _vectorstore is None:
        _vectorstore = build_or_load_vectorstore()
    return _vectorstore.as_retriever(search_kwargs={"k": k})

# LLM(ì–¸ì–´ ëª¨ë¸) ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
def make_llm(model: str = AOAI_DEPLOY_GPT4O_MINI, temperature: float = 0.2) -> AzureChatOpenAI:
    """
    Azure OpenAI ì„œë¹„ìŠ¤ì— ì—°ê²°í•˜ì—¬ LLM(ì–¸ì–´ ëª¨ë¸) ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    Args:
        model (str): ì‚¬ìš©í•  Azure OpenAI ë°°í¬ ëª¨ë¸ì˜ ì´ë¦„. ê¸°ë³¸ê°’ì€ gpt-4o-miniì…ë‹ˆë‹¤.
        temperature (float): ëª¨ë¸ì˜ ì°½ì˜ì„±(ë¬´ì‘ìœ„ì„±)ì„ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜. 0.0ì—ì„œ 2.0 ì‚¬ì´ì˜ ê°’. 
                           ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ì¼ê´€ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    Returns:
        AzureChatOpenAI: ì„¤ì •ëœ ì–¸ì–´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤.
    Raises:
        RuntimeError: Azure OpenAI í™˜ê²½ ë³€ìˆ˜(ì—”ë“œí¬ì¸íŠ¸, API í‚¤)ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ë°œìƒ.
    """
    if not AZURE_AVAILABLE:
        raise RuntimeError("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ LLMì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return AzureChatOpenAI(
        azure_deployment=model,
        api_version=AOAI_API_VERSION,
        azure_endpoint=AOAI_ENDPOINT,
        api_key=AOAI_API_KEY,
        temperature=temperature,
    )
# =============================================================
# 3. LangGraph ë„êµ¬ ë° ë…¸ë“œ ì •ì˜
# ==========================================================
# ìƒíƒœ ê´€ë¦¬ (State Management)
class BotState(TypedDict):
    question: str; intent: str; result: str
    sources: List[Dict[str, Any]]; tool_output: Dict[str, Any]
# ë„êµ¬(Tool) í•¨ìˆ˜
def tool_reset_password(payload: Dict[str, Any]) -> Dict[str, Any]:
    """ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤."""
    return {
        "ok": True, 
        "message": "ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨ ì•ˆë‚´", 
        "steps": ["SSO í¬í„¸ ì ‘ì† > ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •", "ë³¸ì¸ì¸ì¦", "ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •"]
    }

def tool_request_id(payload: Dict[str, Any]) -> Dict[str, Any]:
    """ID ë°œê¸‰ ì‹ ì²­ ì ˆì°¨ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤."""
    return {
        "ok": True, 
        "message": "ID ë°œê¸‰ ì‹ ì²­ ì ˆì°¨ ì•ˆë‚´", 
        "steps": ["HR í¬í„¸ ì ‘ì† > 'ê³„ì • ì‹ ì²­' ì–‘ì‹ ì œì¶œ", "ì–‘ì‹ ìŠ¹ì¸ í›„ ITíŒ€ì—ì„œ ê³„ì • ìƒì„±"]
    }

def tool_owner_lookup(payload: Dict[str, Any]) -> Dict[str, Any]:
    screen = payload.get("screen") or ""
    info = OWNER_FALLBACK.get(screen)
    if not info:
        return {"ok": False, "message": f"'{screen}' ë‹´ë‹¹ì ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
    return {"ok": True, "screen": screen, "owner": info}

# ë…¸ë“œ(Node) í•¨ìˆ˜
# Prompt Engineering - ì‚¬ìš©ì ì˜ë„ ë¶„ì„, ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ì¼ê´€ëœ ì‘ë‹µì„ ë„ì¶œí•˜ë„ë¡ ì„¤ê³„ (í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš©ì„±) [checklist: 2]
def node_classify(state: BotState) -> BotState:
    llm = make_llm()
    sys_prompt = ("ë‹¹ì‹ ì€ ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ë¼ìš°í„°ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì…ë ¥ì„ reset_password, request_id, owner_lookup, rag_qa ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”. JSON(intent, arguments)ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”.")
    msg = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": state["question"]}]
    out = llm.invoke(msg).content
    intent, args = "rag_qa", {}
    try:
        data = json.loads(out)
        intent = data.get("intent", "rag_qa")
        args = data.get("arguments", {}) or {}
    except json.JSONDecodeError:
        logger.warning(f"[Supervisor JSON ì˜¤ë¥˜] JSONDecodeError: {out}")
    except Exception:
        logger.error(f"[Supervisor ì˜¤ë¥˜] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {out}")
    return {**state, "intent": intent, "tool_output": args}

def node_reset_pw(state: BotState) -> BotState: return {**state, "tool_output": tool_reset_password(state.get("tool_output", {}))}

def node_request_id(state: BotState) -> BotState: return {**state, "tool_output": tool_request_id(state.get("tool_output", {}))}

def node_owner_lookup(state: BotState) -> BotState: return {**state, "tool_output": tool_owner_lookup(state.get("tool_output", {}))}

# RAG - ì‚¬ì „ ì •ì˜ëœ ë°ì´í„°(ë¬¸ì„œ)ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIì˜ ë…¼ë¦¬ë ¥ì„ ë³´ê°•/ RAG ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰ ê¸°ëŠ¥ êµ¬í˜„ [checklist: 8,9] 
# Prompt Engineering - í”„ë¡¬í”„íŠ¸ ìµœì í™” (ì—­í•  ë¶€ì—¬ + Chain-of-Thought) [checklist: 1] 
def node_rag(state: BotState) -> BotState:
    docs = retriever(k=4).get_relevant_documents(state["question"])
    context = "\n\n".join([f"[{i+1}] {d.page_content[:1200]}" for i, d in enumerate(docs)])
    sources = [{"index": i+1, "source": d.metadata.get("source","unknown"), "page": d.metadata.get("page")} for i,d in enumerate(docs)]
    llm = make_llm(model=AOAI_DEPLOY_GPT4O)
    sys_prompt = "ë„ˆëŠ” ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ìƒë‹´ì›ì´ë‹¤. ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ë¼."
    user_prompt = f"ì§ˆë¬¸:\n{state['question']}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}"
    out = llm.invoke([{"role":"system","content":sys_prompt},{"role":"user","content":user_prompt}]).content
    return {**state, "result": out, "sources": sources}

# ë„êµ¬(Tool) í•¨ìˆ˜ ê²°ê³¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•íƒœë¡œ ë³€í™˜
def node_finalize(state: BotState) -> BotState:
    if state["intent"] in ["reset_password", "request_id", "owner_lookup"]:
        res = state.get("tool_output", {})
        if state["intent"] == "reset_password":
            text = f"âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i,s in enumerate(res.get("steps", []))) if res.get("ok") else f"â—{res.get('message','ì‹¤íŒ¨')}"
        elif state["intent"] == "request_id":
            text = f"ğŸ†” ID ë°œê¸‰ ì‹ ì²­\nìƒíƒœ: {'ì ‘ìˆ˜ë¨' if res.get('ok') else 'ì‹¤íŒ¨'}\ní‹°ì¼“: {res.get('ticket','-')}"
        else:
            text = f"ğŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ì\n- ì´ë¦„: {res.get('owner', {}).get('owner')}\n- ì´ë©”ì¼: {res.get('owner', {}).get('email')}\n- ì—°ë½ì²˜: {res.get('owner', {}).get('phone')}" if res.get("ok") else f"â—{res.get('message','ì¡°íšŒ ì‹¤íŒ¨')}"
        return {**state, "result": text}
    return state

# LangChain & LangGraph - Multi Agent í˜•íƒœì˜ Agent Flow ì„¤ê³„ ë° êµ¬í˜„/ ReAct (Reasoning and Acting) ì‚¬ìš©/ ë©€í‹°í„´ ëŒ€í™” (memory) [checklist: 3,4,5]
_memory_checkpointer = MemorySaver()
_graph = None
def build_graph():
    # StateGraph í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•¨
    g = StateGraph(BotState)
    g.add_node("classify", node_classify)
    g.add_node("reset_password", node_reset_pw)
    g.add_node("request_id", node_request_id)
    g.add_node("owner_lookup", node_owner_lookup)
    g.add_node("rag", node_rag)
    g.add_node("finalize", node_finalize)
    g.set_entry_point("classify")
    g.add_conditional_edges("classify", lambda s: s["intent"], {"reset_password":"finalize", "request_id":"finalize", "owner_lookup":"finalize", "rag":"rag"})
    g.add_edge("finalize", END); g.add_edge("rag", END)
    return g.compile(checkpointer=_memory_checkpointer)

# =============================================================
# 4. Fallback & Main Pipelines
# =============================================================
_faq_data = None
def load_faq_data() -> List[Dict[str, str]]:
    """kb_default/faq_data.csv íŒŒì¼ì„ ì½ì–´ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤."""
    global _faq_data
    if _faq_data is not None:
        return _faq_data
    
    faq_file_path = KB_DEFAULT_DIR / "faq_data.csv"
    if not faq_file_path.exists():
        _faq_data = []
        return _faq_data
    
    loaded_data = []
    try:
        with open(faq_file_path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # FAQ ì§ˆë¬¸ì„ ë¯¸ë¦¬ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ì €ì¥
                row["faq_words"] = set(okt.phrases(row.get("question", "")))
                loaded_data.append(row)
        logger.info(f"{len(loaded_data)}ê°œì˜ FAQ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"FAQ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    _faq_data = loaded_data
    return _faq_data

def find_similar_faq(question: str) -> Optional[str]:
    """ìì¹´ë“œ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•´ ê°€ì¥ ë¹„ìŠ·í•œ FAQ ì§ˆë¬¸ì„ ì°¾ê³  ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    faq_data = load_faq_data()
    if not faq_data:
        return None

    # ì‚¬ìš©ì ì§ˆë¬¸ì„ í˜•íƒœì†Œ ë¶„ì„
    user_words = set(okt.phrases(question.lower()))
    
    # ë§Œì•½ phrases()ê°€ ì œëŒ€ë¡œ ëœ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬
    # ëª…ì‚¬ ì¶”ì¶œì„ ì¶”ê°€ë¡œ ì‹œë„í•˜ì—¬ ë³´ê°•í•©ë‹ˆë‹¤.
    if not user_words:
        user_words = set(okt.nouns(question.lower()))

    if not user_words:
        return None
    
    best_score = 0.0
    best_answer = None
    
    for item in faq_data:
        faq_words = item.get("faq_words", set())
        
        if not faq_words:
            continue
        
        intersection = len(user_words.intersection(faq_words))
        union = len(user_words.union(faq_words))
        score = intersection / union if union > 0 else 0
        
        if score > best_score:
            best_score = score
            best_answer = item.get("answer")

    if best_score > 0.2:
        return best_answer
    return None

def fallback_pipeline(question: str) -> Dict[str, Any]:
    """í‚¤ì›Œë“œ ë§¤ì¹­ ë° FAQ ê²€ìƒ‰ì„ í†µí•´ ê°„ë‹¨í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í´ë°± í•¨ìˆ˜"""
    logger.info("fallback_pipeline_in", extra={"extra_data": {"q": question}})

    faq_answer = find_similar_faq(question)
    if faq_answer:
        prefix_message = PREFIX_MESSAGES["ok"]
        return {
            "result": prefix_message + faq_answer,
            "intent": "faq",
            "sources": [{"source": "faq_data.csv"}]
        }

    q = question.lower()
    if "ë¹„ë°€ë²ˆí˜¸" in q or "ì´ˆê¸°í™”" in q:
        prefix_message = PREFIX_MESSAGES["ok"]
        intent = "reset_password"
        tool_output = tool_reset_password({})
    elif "id" in q or "ê³„ì •" in q or "ì•„ì´ë””" in q or "ë°œê¸‰" in q:
        prefix_message = PREFIX_MESSAGES["ok"]
        intent = "request_id"
        tool_output = tool_request_id({})
    elif "ë‹´ë‹¹ì" in q:
        prefix_message = PREFIX_MESSAGES["ok"]
        screen = ""
        if "ì¸ì‚¬ì‹œìŠ¤í…œ" in q: screen = "ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬"
        elif "ì¬ë¬´ì‹œìŠ¤í…œ" in q: screen = "ì¬ë¬´ì‹œìŠ¤í…œ-ì •ì‚°í™”ë©´"
        elif "í¬í„¸" in q: screen = "í¬í„¸-ê³µì§€ì‘ì„±"
        
        intent = "owner_lookup"
        if screen:
            tool_output = tool_owner_lookup({"screen": screen})
            res = tool_output
            text = f"ğŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ì\n- ì´ë¦„: {res.get('owner', {}).get('owner')}\n- ì´ë©”ì¼: {res.get('owner', {}).get('email')}\n- ì—°ë½ì²˜: {res.get('owner', {}).get('phone')}" if res.get("ok") else f"â—{res.get('message','ì¡°íšŒ ì‹¤íŒ¨')}"
        else: # ë‹´ë‹¹ì ì¡°íšŒë§Œ ìš”ì²­í–ˆì„ ê²½ìš°
            all_owners_text = "âœ¨ **ë‹´ë‹¹ì ì¡°íšŒ ê°€ëŠ¥ ëª©ë¡** âœ¨\n\n"
            for s, info in OWNER_FALLBACK.items():
                all_owners_text += f"**- {s.split('-')[0]} ë‹´ë‹¹ì:** {info.get('owner')}\n"
            all_owners_text += "\n\n**Tip:** 'ì¸ì‚¬ì‹œìŠ¤í…œ ë‹´ë‹¹ì ëˆ„êµ¬ì•¼?'ì²˜ëŸ¼ êµ¬ì²´ì ì¸ ì‹œìŠ¤í…œëª…ì„ ì…ë ¥í•˜ë©´ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            text = all_owners_text
            return {"result": prefix_message + text, "intent": intent, "sources": []}
    else:
        prefix_message = PREFIX_MESSAGES["fail"]
        no_match_message = "ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤.\nì§€ì›ë˜ëŠ” ê¸°ëŠ¥ê³¼ ê´€ë ¨ëœ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, ì¶”ê°€ ë¬¸ì˜ëŠ” ê³ ê°ì„¼í„°ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”."
        return {
            "result": prefix_message + no_match_message,
            "intent": "fallback_no_match",
            "sources": []
        }

    res = tool_output
    if intent == "reset_password":
        text = f"âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i,s in enumerate(res.get("steps", []))) if res.get("ok") else f"â—{res.get('message','ì‹¤íŒ¨')}"
    elif intent == "request_id":
        text = f"ğŸ†” ID ë°œê¸‰ ì‹ ì²­\n\n" + "\n".join(f"{i+1}. {s}" for i,s in enumerate(res.get("steps", []))) if res.get("ok") else f"â—{res.get('message','ì‹¤íŒ¨')}"
    else:
        text = f"ğŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ì\n- ì´ë¦„: {res.get('owner', {}).get('owner')}\n- ì´ë©”ì¼: {res.get('owner', {}).get('email')}\n- ì—°ë½ì²˜: {res.get('owner', {}).get('phone')}" if res.get("ok") else f"â—{res.get('message','ì¡°íšŒ ì‹¤íŒ¨')}"

    return {"result": prefix_message + text, "intent": intent, "sources": []}

_graph = None
def run_graph_pipeline(question: str, session_id: str) -> Dict[str, Any]:
    # [checklist: 5] LangChain & LangGraph - ë©€í‹°í„´ ëŒ€í™” (memory) í™œìš©
    """LangGraph ê¸°ë°˜ì˜ AI íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    global _graph
    logger.info("pipeline_in", extra={"extra_data": {"q": question}})
    if _graph is None: _graph = build_graph()
    out = _graph.invoke(
        input={"question": question, "intent":"", "result":"", "sources":[], "tool_output":{}},
        config={"configurable": {"thread_id": session_id}}
    )
    logger.info("pipeline_out", extra={"extra_data": {"intent": out.get("intent", "")}})
    return out

def pipeline(question: str, session_id: str) -> Dict[str, Any]:
    """
    Azure ì—°ê²° ìƒíƒœì— ë”°ë¼ ì ì ˆí•œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìš”ì²­ì„ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    """
    corrected_question = question
    
    # ê°„ë‹¨í•œ ì¸ì‚¬ë§ì— ëŒ€í•œ ì‘ë‹µ ì²˜ë¦¬
    if corrected_question.lower().strip() in GREETINGS:
        return {
            "result": "ë„¤ ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬¸ì˜ì‚¬í•­ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ì œê°€ ë„ì™€ë“œë¦´ê²Œìš”.",
            "intent": "greeting",
            "sources": []
        }

    if AZURE_AVAILABLE:
        return run_graph_pipeline(corrected_question, session_id)
    else:
        return fallback_pipeline(corrected_question)